#!/bin/env python3

import multiprocessing
import dataclasses
import threading
import requests
import platform
import pathlib
import tomllib
import fnmatch
import dacite
import lzma
import time
import zlib
import sys

import urllib.parse

from datetime import datetime
from typing import Optional


machine = platform.machine()
match machine:
    case "i686": machine = "i386"
    case "x86_64": machine = "amd64"
    case "armv7l": machine = "armhf"
    case "aarch64": machine = "arm64"
    case _: raise ValueError(machine)

DEBIAN_BINARY = f"binary-{machine}"

# @todo: Don't output in critical section. Use queue.
output_lock = threading.Lock()


@dataclasses.dataclass
class PackageSpec():
    url: Optional[str]
    name: Optional[str]
    not_deps: Optional[list[str]]
    version: Optional[str]
    last_modified: Optional[str | datetime]
    deps: Optional[list[str]]


@dataclasses.dataclass
class GithubAsset():
    name: str
    updated_at: str
    browser_download_url: str


@dataclasses.dataclass
class GithubSource(PackageSpec):
    repository: str
    pattern: str


@dataclasses.dataclass
class DebianSource():
    base_url: str
    distribution: str
    components: list[str]
    packages: Optional[dict[str, PackageSpec]]
    whitelist: Optional[list[str]]
    blacklist: Optional[list[str]]


@dataclasses.dataclass
class GithubRelease():
    assets: list[GithubAsset]


@dataclasses.dataclass
class Config():
    github_token: Optional[str]
    sources: Optional[dict[str, DebianSource | GithubSource | PackageSpec]]


def parse_debian_package(string: bytes | str) -> list[dict[str, str]]:
    if isinstance(string, bytes):
        string = string.decode()

    assert isinstance(string, str)

    packages = list()
    for package_string in string.split("\n\n"):
        if len(package_string) == 0: continue

        package = dict()
        for entry_string in package_string.split("\n"):
            # skip continutations
            if entry_string[0].isspace(): continue

            key, value = entry_string.split(": ", 1)
            package[key] = value

        packages.append(package)

    return packages


def print_package(p: PackageSpec):
    global output_lock

    with output_lock:
        print(f"[{p.name}]")

        if p.url is not None:
            print(f"url = {p.url}")

        if p.deps is not None:
            print(f"deps = {' '.join(p.deps)}")

        if p.not_deps is not None:
            print(f"not_deps = {' '.join(p.not_deps)}")

        if p.version is not None:
            print(f"version = {p.version}")

        if p.last_modified is not None:
            assert isinstance(p.last_modified, datetime)

            last_modified_unix = int(time.mktime(p.last_modified.timetuple()))
            print(f"last_modified = {last_modified_unix}")

        print()


def process_single_source(name, source):
    global machine
    print(f"Syncing {name}", file=sys.stderr)

    if isinstance(source, DebianSource):
        parts = urllib.parse.urlparse(source.base_url)
        base_path = pathlib.Path(parts.path if parts.path else "/")
        distribution_path = base_path/source.distribution
        dists_path = distribution_path/"dists"
        binary_path = dists_path/'/'.join(source.components)/DEBIAN_BINARY

        try_fetch = [(binary_path/"Packages.xz", lambda d:
                      lzma.decompress(d)),
                     (binary_path/"Packages.gz", lambda d:
                      zlib.decompress(d, 16+zlib.MAX_WBITS)),
                     (binary_path/"Packages",
                      lambda d: d)]
        for path, function in try_fetch:
            package_url = urllib.parse.urlunparse((parts.scheme,
                                                   parts.netloc,
                                                   path.as_posix(),
                                                   "",
                                                   "",
                                                   ""))
            response = requests.get(package_url)

            if response.status_code == 200:
                content = function(response.content)
                break
        else:
            print(f"{name}: request failed", file=sys.stderr)
            return

        for item in parse_debian_package(content):
            k, v = item["Package"], item
            if source.whitelist is not None and k not in source.whitelist:
                continue

            if source.blacklist is not None and k in source.blacklist:
                continue

            packages = source.packages if source.packages is not None else {}

            deb_path = distribution_path/v["Filename"]
            deb_url = urllib.parse.urlunparse((parts.scheme,
                                               parts.netloc,
                                               deb_path.as_posix(),
                                               "",
                                               "",
                                               ""))

            p = packages.setdefault(k, PackageSpec(name=None,
                                                   not_deps=None,
                                                   deps=None,
                                                   url=None,
                                                   last_modified=None,
                                                   version=None))
            p.name = p.name if p.name is not None else k
            p.url = p.url if p.url is not None else deb_url
            p.version = p.version if p.version is not None else v["Version"]
            print_package(p)
    elif isinstance(source, GithubSource):
        path = pathlib.Path("/")/"repos"/source.repository/"releases"/"latest"
        url = urllib.parse.urlunparse(("https",
                                       "api.github.com",
                                       path.as_posix(),
                                       "",
                                       "",
                                       ""))

        response = requests.get(url)
        release = dacite.from_dict(data_class=GithubRelease,
                                   data=response.json())

        asset = None
        for asset_ in release.assets:
            if fnmatch.fnmatch(asset_.name, source.pattern):
                asset = asset_

        assert asset is not None

        t = datetime.strptime(asset.updated_at, '%Y-%m-%dT%H:%M:%SZ')

        if source.name is None:
            source.name = name

        if source.url is None:
            source.url = asset.browser_download_url

        if source.last_modified is None:
            source.last_modified = t

        print_package(source)
    elif isinstance(source, PackageSpec):
        if source.url is None:
            print(f"{name}: No URL, skipping.", file=sys.stderr)
            return

        response = requests.head(source.url, allow_redirects=True)
        t = datetime.strptime(response.headers["last-modified"],
                              "%a, %d %b %Y %H:%M:%S %Z")

        if source.name is None:
            source.name = name

        if source.last_modified is None:
            source.last_modified = t

        print_package(source)


def thread_fn(begin, length):
    assert config.sources is not None

    for name, source in list(config.sources.items())[begin:begin+length]:
        process_single_source(name, source)


if __name__ == '__main__':
    p = tomllib.load(open("vpkg-sync.toml", "rb"))
    config = dacite.from_dict(data_class=Config, data=p)

    if config.sources is None:
        exit(0)

    try:
        numthreads = multiprocessing.cpu_count()
    except NotImplementedError:
        numthreads = 1

    numitems = len(config.sources)
    if numthreads > numitems:
        numthreads = numitems

    if numthreads == 1:
        for name, source in config.sources.items():
            process_single_source(name, source)
    else:
        for i in range(0, numthreads):
            size = (numitems + (numthreads - 1)) // numthreads
            t = threading.Thread(target=thread_fn, args=(i*size, size))
            t.start()
